{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "109423005 Data_Science_and_Machine_Learning_pretest_v21_02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4MpIDbALpZ9"
      },
      "source": [
        "# 2021 Data Science and Machine Learning Pretest\n",
        "\n",
        "*   This Colab is read-only. Please save a copy of it on your Drive to edit it by going to `Menu > File > Save a copy in Drive`.\n",
        "*   Rename your Colab in the following format and replace 109423000 with your student ID:\n",
        "> `Copy 109423000 of Data Science and Machine Learning pretest v21.02.ipynb`\n",
        "*   You are required to complete this pretest **on your own**.\n",
        "*   When you have completed the questions below, make sure you turn on the **share/view**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOY37mmch2f9"
      },
      "source": [
        "# Question 1: Text preprocessing\n",
        "Most of the text data acquired through web crawling and review can be noisy. When handling this kind of text data, preprocessing is an important step to ensure the quality of the dataset. There are multiple ways of doing text preprocessing. Below is an example flow of preprocessing text data. \n",
        "1. lowercase \n",
        "2. decontracting \n",
        "3. remove tags, punctuations, numbers\n",
        "3. tokenization \n",
        "4. stopword removal \n",
        "5. lemmatization \n",
        "6. stemming\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Nfz6oe2DenL6",
        "outputId": "a7b8c28d-af43-4e33-950d-cad2143a81c8"
      },
      "source": [
        "lowercase=將句子統一成小寫\n",
        "decontracting=將縮寫還原，像 don't--> do not,i've-->i have\n",
        "remove tags, punctuations, numbers=去除無用標籤（例如html內的tag)、標點符號、號碼\n",
        "tokenization=分詞，像是把 \"i am dog\"-->\"i\",\"am\",\"dog\"\n",
        "stopword removal=去除停用詞，像是the,this,is\n",
        "lemmatization=詞形還原，同義詞替換的就替換掉，他相較於stemmimg 比較能留下一個含義相同的有效詞語\n",
        "stemming=詞幹提取，例如leafs跟leaves，lemmatization會把他們一起替換成leaf，但stemming 是把他們變成leaf,leav(就非有效詞語)\n",
        "#source:https://www.itread01.com/content/1546991112.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9bd526dc1bd8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    remove tags, punctuations, numbers=去除無用標籤、標點符號、號碼\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSywRvPDjNWD"
      },
      "source": [
        "## 1-1. Please briefly explain what each step is doing.(30%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXCRRo8WlHbU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP3t4W6flEIC"
      },
      "source": [
        "## 1-2. Please use the sample data and do the preprocessing following the provided flow.(70%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7n8-qVplIrU"
      },
      "source": [
        "documents = [\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as 'Jumbo'\",\n",
        "        \"I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE , HE CAME OUT OF HIS STORAGE ROOM WITH ( A PACKET OF McCANNS INSTANT IRISH OATMEAL .) HE SUGGESTED THAT I TRY IT FOR MY OWN USE ,IN MY STASH . SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO , SO I ENDED UP TRYING THE APPLE AND CINN . FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK . IT GOES GOOD WITH O.J. AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLD...OR THE DAY AT LEAST..  JERRY REITH...\",\n",
        "        \"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\",\n",
        "        \"Product received is as advertised.<br /><br /><a href='http://www.amazon.com/gp/product/B001GVISJM'>Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)</a>\",\n",
        "        \"this was sooooo deliscious but too bad i ate em too fast and gained 2 pds! my fault\",\n",
        "        \"Deal was awesome!  Arrived before Halloween as indicated and was enough to satisfy trick or treaters.  I love the quality of this product and it was much less expensive than the local store's candy.\",\n",
        "        \"I love these.........very tasty!!!!!!!!!!!  Infact, I think I am addicted to them.<br />Buying them in packs of 6 bags - is very reasonable than going to Target and getting a bag.  Savings are about a $1.00 a bag.  I use subscribe and save on these and several other product.  I love subscribe and save!!!!!!!!!!!\",\n",
        "        \"I LOVE spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation doesn't go away for like 3 hours! Not sure if that is healthy or not .... and you can buy this at Walmart for $0.28, way cheaper than Amazon.\",\n",
        "        \"Makes a tasty, super easy meal, fast. BUT high in calories.<br /><br />The instructions say to saute the veggies first but I recommend cooking the chicken first. The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess. I made it with snow peas and carrots only. I dont like the little corn.  Added some red pepper flakes for heat and served ontop of rice.  It came out wonderful! Dinner on the table in less than 30mins.\",\n",
        "        \"Love this sugar.  I also get muscavado sugar and they are both great to use in place of regular white sugar. Recommend!\",\n",
        "        \"This is just Fantastic Chicken Noodle soup, the best I have ever eaten, with large hearty chunks of chicken,and vegetables and nice large noodles. This soup is just so full bodied, and is seasoned just right.  I am so glad Amazon carries this product.  I just can't find it here in Vermont.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39UO3QiRenL8"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "LLEiWlmoqmpY",
        "outputId": "d6d73f17-73f0-4a54-e687-73a1a1fadd3f"
      },
      "source": [
        "df= pd.DataFrame(documents,columns=['sentence'])#sentence是標題\n",
        "#df= pd.DataFrame(documents)\n",
        "df "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I don't know if it's the cactus or the tequila...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this was sooooo deliscious but too bad i ate e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Deal was awesome!  Arrived before Halloween as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I love these.........very tasty!!!!!!!!!!!  In...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I LOVE spicy ramen, but for whatever reasons t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Makes a tasty, super easy meal, fast. BUT high...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Love this sugar.  I also get muscavado sugar a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>This is just Fantastic Chicken Noodle soup, th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             sentence\n",
              "0   Product arrived labeled as Jumbo Salted Peanut...\n",
              "1   I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...\n",
              "2   I don't know if it's the cactus or the tequila...\n",
              "3   Product received is as advertised.<br /><br />...\n",
              "4   this was sooooo deliscious but too bad i ate e...\n",
              "5   Deal was awesome!  Arrived before Halloween as...\n",
              "6   I love these.........very tasty!!!!!!!!!!!  In...\n",
              "7   I LOVE spicy ramen, but for whatever reasons t...\n",
              "8   Makes a tasty, super easy meal, fast. BUT high...\n",
              "9   Love this sugar.  I also get muscavado sugar a...\n",
              "10  This is just Fantastic Chicken Noodle soup, th..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEd44nrMenL9",
        "outputId": "047f8726-60ad-4cb0-d7f9-3dc025fa034a"
      },
      "source": [
        "# 'lowercase' string\n",
        "df['sentence'].str.lower()\n",
        "#source:https://datatofish.com/lowercase-pandas-dataframe/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     product arrived labeled as jumbo salted peanut...\n",
              "1     i was visiting my friend nate the other mornin...\n",
              "2     i don't know if it's the cactus or the tequila...\n",
              "3     product received is as advertised.<br /><br />...\n",
              "4     this was sooooo deliscious but too bad i ate e...\n",
              "5     deal was awesome!  arrived before halloween as...\n",
              "6     i love these.........very tasty!!!!!!!!!!!  in...\n",
              "7     i love spicy ramen, but for whatever reasons t...\n",
              "8     makes a tasty, super easy meal, fast. but high...\n",
              "9     love this sugar.  i also get muscavado sugar a...\n",
              "10    this is just fantastic chicken noodle soup, th...\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "PNaX7c2PenL-",
        "outputId": "80fbd1c8-1604-4409-d25a-c239d6900ccf"
      },
      "source": [
        "# contracted text\n",
        "!pip install contractions\n",
        "import contractions\n",
        "temp= df['sentence'].tolist()#source:https://datatofish.com/convert-pandas-dataframe-to-list/\n",
        "store =[]# creating an empty list \n",
        "for word in str(temp).split():# using contractions.fix to expand the shotened words \n",
        "    store.append(contractions.fix(word))\n",
        "decontracting=' '.join(store)#將元素連接、合併成一個字串\n",
        "decontracting\n",
        "#source:https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \\'Jumbo\\'\", \\'I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE , HE CAME OUT OF HIS STORAGE ROOM WITH ( A PACKET OF McCANNS INSTANT IRISH OATMEAL .) HE SUGGESTED THAT I TRY IT FOR MY OWN USE ,IN MY STASH . SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO , SO I ENDED UP TRYING THE APPLE AND CINN . FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK . IT GOES GOOD WITH of.J. AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLD...OR THE DAY AT LEAST.. JERRY REITH...\\', \"I do not know if it is the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind! We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away! When we realized that we simply could not find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but do not want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan. Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\", \"Product received is as advertised.<br /><br /><a href=\\'http://www.amazon.com/gp/product/B001GVISJM\\'>Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)</a>\", \\'this was sooooo deliscious but too bad i ate them too fast and gained 2 pds! my fault\\', \"Deal was awesome! Arrived before Halloween as indicated and was enough to satisfy trick or treaters. I love the quality of this product and it was much less expensive than the local store\\'s candy.\", \\'I love these.........very tasty!!!!!!!!!!! Infact, I think I am addicted to them.<br />Buying them in packs of 6 bags - is very reasonable than going to Target and getting a bag. Savings are about a $1.00 a bag. I use subscribe and save on these and several other product. I love subscribe and save!!!!!!!!!!!\\', \"I LOVE spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation does not go away for like 3 hours! Not sure if that is healthy or not .... and you can buy this at Walmart for $0.28, way cheaper than Amazon.\", \\'Makes a tasty, super easy meal, fast. BUT high in calories.<br /><br />The instructions say to saute the veggies first but I recommend cooking the chicken first. The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess. I made it with snow peas and carrots only. I do not like the little corn. Added some red pepper flakes for heat and served ontop of rice. It came out wonderful! Dinner on the table in less than 30mins.\\', \\'Love this sugar. I also get muscavado sugar and they are both great to use in place of regular white sugar. Recommend!\\', \"This is just Fantastic Chicken Noodle soup, the best I have ever eaten, with large hearty chunks of chicken,and vegetables and nice large noodles. This soup is just so full bodied, and is seasoned just right. I am so glad Amazon carries this product. I just can not find it here in Vermont.\"]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "huuVmjVGenL_",
        "outputId": "c86db043-ab51-4321-d3f4-bb073bb61ff8"
      },
      "source": [
        "#remove html tags\n",
        "import re\n",
        "reg = re.compile('<[^>]*>')\n",
        "tags=reg.sub('',decontracting)\n",
        "tags\n",
        "#source:https://www.itread01.com/content/1547191643.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \\'Jumbo\\'\", \\'I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE , HE CAME OUT OF HIS STORAGE ROOM WITH ( A PACKET OF McCANNS INSTANT IRISH OATMEAL .) HE SUGGESTED THAT I TRY IT FOR MY OWN USE ,IN MY STASH . SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO , SO I ENDED UP TRYING THE APPLE AND CINN . FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK . IT GOES GOOD WITH of.J. AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLD...OR THE DAY AT LEAST.. JERRY REITH...\\', \"I do not know if it is the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind! We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away! When we realized that we simply could not find it anywhere in our city we were bummed.Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.If you love hot sauce..I mean really love hot sauce, but do not want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan. Just realize that once you taste it, you will never want to use any other sauce.Thank you for the personal, incredible service!\", \"Product received is as advertised.Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)\", \\'this was sooooo deliscious but too bad i ate them too fast and gained 2 pds! my fault\\', \"Deal was awesome! Arrived before Halloween as indicated and was enough to satisfy trick or treaters. I love the quality of this product and it was much less expensive than the local store\\'s candy.\", \\'I love these.........very tasty!!!!!!!!!!! Infact, I think I am addicted to them.Buying them in packs of 6 bags - is very reasonable than going to Target and getting a bag. Savings are about a $1.00 a bag. I use subscribe and save on these and several other product. I love subscribe and save!!!!!!!!!!!\\', \"I LOVE spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation does not go away for like 3 hours! Not sure if that is healthy or not .... and you can buy this at Walmart for $0.28, way cheaper than Amazon.\", \\'Makes a tasty, super easy meal, fast. BUT high in calories.The instructions say to saute the veggies first but I recommend cooking the chicken first. The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess. I made it with snow peas and carrots only. I do not like the little corn. Added some red pepper flakes for heat and served ontop of rice. It came out wonderful! Dinner on the table in less than 30mins.\\', \\'Love this sugar. I also get muscavado sugar and they are both great to use in place of regular white sugar. Recommend!\\', \"This is just Fantastic Chicken Noodle soup, the best I have ever eaten, with large hearty chunks of chicken,and vegetables and nice large noodles. This soup is just so full bodied, and is seasoned just right. I am so glad Amazon carries this product. I just can not find it here in Vermont.\"]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "zK_sq7CbenMA",
        "outputId": "d1d53c41-5f66-4df2-8e2d-7d9c94986b12"
      },
      "source": [
        "#remove punctuation\n",
        "def remove_punctuation(words):\n",
        "#\"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "b=remove_punctuation(tags)\n",
        "c=''.join(b)\n",
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Product arrived labeled as Jumbo Salted Peanutsthe peanuts were actually small sized unsalted Not sure if this was an error or if the vendor intended to represent the product as Jumbo I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE  HE CAME OUT OF HIS STORAGE ROOM WITH  A PACKET OF McCANNS INSTANT IRISH OATMEAL  HE SUGGESTED THAT I TRY IT FOR MY OWN USE IN MY STASH  SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO  SO I ENDED UP TRYING THE APPLE AND CINN  FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK  IT GOES GOOD WITH ofJ AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLDOR THE DAY AT LEAST JERRY REITH I do not know if it is the cactus or the tequila or just the unique combination of ingredients but the flavour of this hot sauce makes it one of a kind We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away When we realized that we simply could not find it anywhere in our city we were bummedNow because of the magic of the internet we have a case of the sauce and are ecstatic because of itIf you love hot sauceI mean really love hot sauce but do not want a sauce that tastelessly burns your throat grab a bottle of Tequila Picante Gourmet de Inclan Just realize that once you taste it you will never want to use any other sauceThank you for the personal incredible service Product received is as advertisedTwizzlers Strawberry 16Ounce Bags Pack of 6 this was sooooo deliscious but too bad i ate them too fast and gained 2 pds my fault Deal was awesome Arrived before Halloween as indicated and was enough to satisfy trick or treaters I love the quality of this product and it was much less expensive than the local stores candy I love thesevery tasty Infact I think I am addicted to themBuying them in packs of 6 bags  is very reasonable than going to Target and getting a bag Savings are about a 100 a bag I use subscribe and save on these and several other product I love subscribe and save I LOVE spicy ramen but for whatever reasons this thing burns my stomach badly and the burning sensation does not go away for like 3 hours Not sure if that is healthy or not  and you can buy this at Walmart for 028 way cheaper than Amazon Makes a tasty super easy meal fast BUT high in caloriesThe instructions say to saute the veggies first but I recommend cooking the chicken first The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess I made it with snow peas and carrots only I do not like the little corn Added some red pepper flakes for heat and served ontop of rice It came out wonderful Dinner on the table in less than 30mins Love this sugar I also get muscavado sugar and they are both great to use in place of regular white sugar Recommend This is just Fantastic Chicken Noodle soup the best I have ever eaten with large hearty chunks of chickenand vegetables and nice large noodles This soup is just so full bodied and is seasoned just right I am so glad Amazon carries this product I just can not find it here in Vermont'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "iLHmsBwzenMA",
        "outputId": "0127aeab-bdbc-40b1-a02e-9c697c49d922"
      },
      "source": [
        "# replace number\n",
        "import re\n",
        "pattern = r'[0-9]'\n",
        "# Match all digits in the string and replace them by empty string\n",
        "remove2 = re.sub(pattern, '', c)\n",
        "remove2\n",
        "#source:https://thispointer.com/python-remove-all-numbers-from-string/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Product arrived labeled as Jumbo Salted Peanutsthe peanuts were actually small sized unsalted Not sure if this was an error or if the vendor intended to represent the product as Jumbo I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE  HE CAME OUT OF HIS STORAGE ROOM WITH  A PACKET OF McCANNS INSTANT IRISH OATMEAL  HE SUGGESTED THAT I TRY IT FOR MY OWN USE IN MY STASH  SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO  SO I ENDED UP TRYING THE APPLE AND CINN  FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK  IT GOES GOOD WITH ofJ AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLDOR THE DAY AT LEAST JERRY REITH I do not know if it is the cactus or the tequila or just the unique combination of ingredients but the flavour of this hot sauce makes it one of a kind We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away When we realized that we simply could not find it anywhere in our city we were bummedNow because of the magic of the internet we have a case of the sauce and are ecstatic because of itIf you love hot sauceI mean really love hot sauce but do not want a sauce that tastelessly burns your throat grab a bottle of Tequila Picante Gourmet de Inclan Just realize that once you taste it you will never want to use any other sauceThank you for the personal incredible service Product received is as advertisedTwizzlers Strawberry Ounce Bags Pack of  this was sooooo deliscious but too bad i ate them too fast and gained  pds my fault Deal was awesome Arrived before Halloween as indicated and was enough to satisfy trick or treaters I love the quality of this product and it was much less expensive than the local stores candy I love thesevery tasty Infact I think I am addicted to themBuying them in packs of  bags  is very reasonable than going to Target and getting a bag Savings are about a  a bag I use subscribe and save on these and several other product I love subscribe and save I LOVE spicy ramen but for whatever reasons this thing burns my stomach badly and the burning sensation does not go away for like  hours Not sure if that is healthy or not  and you can buy this at Walmart for  way cheaper than Amazon Makes a tasty super easy meal fast BUT high in caloriesThe instructions say to saute the veggies first but I recommend cooking the chicken first The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess I made it with snow peas and carrots only I do not like the little corn Added some red pepper flakes for heat and served ontop of rice It came out wonderful Dinner on the table in less than mins Love this sugar I also get muscavado sugar and they are both great to use in place of regular white sugar Recommend This is just Fantastic Chicken Noodle soup the best I have ever eaten with large hearty chunks of chickenand vegetables and nice large noodles This soup is just so full bodied and is seasoned just right I am so glad Amazon carries this product I just can not find it here in Vermont'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3WzYO8JenMB",
        "outputId": "2efa711f-a2a3-41f8-cf85-2c895537e774"
      },
      "source": [
        "#tokenization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# 下載分詞模組\"Punkt Tokenizer Models\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjMiuEbgenMC",
        "outputId": "4317fac6-13c4-44fe-da3e-7d8790c23178"
      },
      "source": [
        "# 分句，將段落切成句子，遇到句點就斷詞\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.sent_tokenize(decontracting)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted.',\n",
              " 'Not sure if this was an error or if the vendor intended to represent the product as \\'Jumbo\\'\", \\'I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE , HE CAME OUT OF HIS STORAGE ROOM WITH ( A PACKET OF McCANNS INSTANT IRISH OATMEAL .)',\n",
              " 'HE SUGGESTED THAT I TRY IT FOR MY OWN USE ,IN MY STASH .',\n",
              " 'SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO , SO I ENDED UP TRYING THE APPLE AND CINN .',\n",
              " 'FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK .',\n",
              " 'IT GOES GOOD WITH of.J.',\n",
              " 'AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLD...OR THE DAY AT LEAST.. JERRY REITH...\\', \"I do not know if it is the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!',\n",
              " 'We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!',\n",
              " 'When we realized that we simply could not find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but do not want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.',\n",
              " 'Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!',\n",
              " '\", \"Product received is as advertised.<br /><br /><a href=\\'http://www.amazon.com/gp/product/B001GVISJM\\'>Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)</a>\", \\'this was sooooo deliscious but too bad i ate them too fast and gained 2 pds!',\n",
              " 'my fault\\', \"Deal was awesome!',\n",
              " 'Arrived before Halloween as indicated and was enough to satisfy trick or treaters.',\n",
              " \"I love the quality of this product and it was much less expensive than the local store's candy.\",\n",
              " '\", \\'I love these.........very tasty!!!!!!!!!!!',\n",
              " 'Infact, I think I am addicted to them.<br />Buying them in packs of 6 bags - is very reasonable than going to Target and getting a bag.',\n",
              " 'Savings are about a $1.00 a bag.',\n",
              " 'I use subscribe and save on these and several other product.',\n",
              " 'I love subscribe and save!!!!!!!!!!!',\n",
              " '\\', \"I LOVE spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation does not go away for like 3 hours!',\n",
              " 'Not sure if that is healthy or not .... and you can buy this at Walmart for $0.28, way cheaper than Amazon.',\n",
              " '\", \\'Makes a tasty, super easy meal, fast.',\n",
              " 'BUT high in calories.<br /><br />The instructions say to saute the veggies first but I recommend cooking the chicken first.',\n",
              " 'The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess.',\n",
              " 'I made it with snow peas and carrots only.',\n",
              " 'I do not like the little corn.',\n",
              " 'Added some red pepper flakes for heat and served ontop of rice.',\n",
              " 'It came out wonderful!',\n",
              " 'Dinner on the table in less than 30mins.',\n",
              " \"', 'Love this sugar.\",\n",
              " 'I also get muscavado sugar and they are both great to use in place of regular white sugar.',\n",
              " 'Recommend!',\n",
              " '\\', \"This is just Fantastic Chicken Noodle soup, the best I have ever eaten, with large hearty chunks of chicken,and vegetables and nice large noodles.',\n",
              " 'This soup is just so full bodied, and is seasoned just right.',\n",
              " 'I am so glad Amazon carries this product.',\n",
              " 'I just can not find it here in Vermont.\"]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73lzSa3nenMC",
        "outputId": "7e951961-d637-4b7c-b094-7c3cdfbccfdb"
      },
      "source": [
        "# 分詞，將句子切成單字，遇到空白或符號就分詞\n",
        "tokenization=nltk.word_tokenize(decontracting)\n",
        "tokenization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " '``',\n",
              " 'Product',\n",
              " 'arrived',\n",
              " 'labeled',\n",
              " 'as',\n",
              " 'Jumbo',\n",
              " 'Salted',\n",
              " 'Peanuts',\n",
              " '...',\n",
              " 'the',\n",
              " 'peanuts',\n",
              " 'were',\n",
              " 'actually',\n",
              " 'small',\n",
              " 'sized',\n",
              " 'unsalted',\n",
              " '.',\n",
              " 'Not',\n",
              " 'sure',\n",
              " 'if',\n",
              " 'this',\n",
              " 'was',\n",
              " 'an',\n",
              " 'error',\n",
              " 'or',\n",
              " 'if',\n",
              " 'the',\n",
              " 'vendor',\n",
              " 'intended',\n",
              " 'to',\n",
              " 'represent',\n",
              " 'the',\n",
              " 'product',\n",
              " 'as',\n",
              " \"'Jumbo\",\n",
              " \"'\",\n",
              " \"''\",\n",
              " ',',\n",
              " \"'I\",\n",
              " 'WAS',\n",
              " 'VISITING',\n",
              " 'MY',\n",
              " 'FRIEND',\n",
              " 'NATE',\n",
              " 'THE',\n",
              " 'OTHER',\n",
              " 'MORNING',\n",
              " 'FOR',\n",
              " 'COFFEE',\n",
              " ',',\n",
              " 'HE',\n",
              " 'CAME',\n",
              " 'OUT',\n",
              " 'OF',\n",
              " 'HIS',\n",
              " 'STORAGE',\n",
              " 'ROOM',\n",
              " 'WITH',\n",
              " '(',\n",
              " 'A',\n",
              " 'PACKET',\n",
              " 'OF',\n",
              " 'McCANNS',\n",
              " 'INSTANT',\n",
              " 'IRISH',\n",
              " 'OATMEAL',\n",
              " '.',\n",
              " ')',\n",
              " 'HE',\n",
              " 'SUGGESTED',\n",
              " 'THAT',\n",
              " 'I',\n",
              " 'TRY',\n",
              " 'IT',\n",
              " 'FOR',\n",
              " 'MY',\n",
              " 'OWN',\n",
              " 'USE',\n",
              " ',',\n",
              " 'IN',\n",
              " 'MY',\n",
              " 'STASH',\n",
              " '.',\n",
              " 'SOMETIMES',\n",
              " 'NATE',\n",
              " 'DOSE',\n",
              " 'NOT',\n",
              " 'GIVE',\n",
              " 'YOU',\n",
              " 'A',\n",
              " 'CHANCE',\n",
              " 'TO',\n",
              " 'SAY',\n",
              " 'NO',\n",
              " ',',\n",
              " 'SO',\n",
              " 'I',\n",
              " 'ENDED',\n",
              " 'UP',\n",
              " 'TRYING',\n",
              " 'THE',\n",
              " 'APPLE',\n",
              " 'AND',\n",
              " 'CINN',\n",
              " '.',\n",
              " 'FOUND',\n",
              " 'IT',\n",
              " 'TO',\n",
              " 'BE',\n",
              " 'VERY',\n",
              " 'TASTEFULL',\n",
              " 'WHEN',\n",
              " 'MADE',\n",
              " 'WITH',\n",
              " 'WATER',\n",
              " 'OR',\n",
              " 'POWDERED',\n",
              " 'MILK',\n",
              " '.',\n",
              " 'IT',\n",
              " 'GOES',\n",
              " 'GOOD',\n",
              " 'WITH',\n",
              " 'of.J',\n",
              " '.',\n",
              " 'AND',\n",
              " 'COFFEE',\n",
              " 'AND',\n",
              " 'A',\n",
              " 'SLICE',\n",
              " 'OF',\n",
              " 'TOAST',\n",
              " 'AND',\n",
              " 'YOUR',\n",
              " 'READY',\n",
              " 'TO',\n",
              " 'TAKE',\n",
              " 'ON',\n",
              " 'THE',\n",
              " 'WORLD',\n",
              " '...',\n",
              " 'OR',\n",
              " 'THE',\n",
              " 'DAY',\n",
              " 'AT',\n",
              " 'LEAST..',\n",
              " 'JERRY',\n",
              " 'REITH',\n",
              " '...',\n",
              " \"'\",\n",
              " ',',\n",
              " '``',\n",
              " 'I',\n",
              " 'do',\n",
              " 'not',\n",
              " 'know',\n",
              " 'if',\n",
              " 'it',\n",
              " 'is',\n",
              " 'the',\n",
              " 'cactus',\n",
              " 'or',\n",
              " 'the',\n",
              " 'tequila',\n",
              " 'or',\n",
              " 'just',\n",
              " 'the',\n",
              " 'unique',\n",
              " 'combination',\n",
              " 'of',\n",
              " 'ingredients',\n",
              " ',',\n",
              " 'but',\n",
              " 'the',\n",
              " 'flavour',\n",
              " 'of',\n",
              " 'this',\n",
              " 'hot',\n",
              " 'sauce',\n",
              " 'makes',\n",
              " 'it',\n",
              " 'one',\n",
              " 'of',\n",
              " 'a',\n",
              " 'kind',\n",
              " '!',\n",
              " 'We',\n",
              " 'picked',\n",
              " 'up',\n",
              " 'a',\n",
              " 'bottle',\n",
              " 'once',\n",
              " 'on',\n",
              " 'a',\n",
              " 'trip',\n",
              " 'we',\n",
              " 'were',\n",
              " 'on',\n",
              " 'and',\n",
              " 'brought',\n",
              " 'it',\n",
              " 'back',\n",
              " 'home',\n",
              " 'with',\n",
              " 'us',\n",
              " 'and',\n",
              " 'were',\n",
              " 'totally',\n",
              " 'blown',\n",
              " 'away',\n",
              " '!',\n",
              " 'When',\n",
              " 'we',\n",
              " 'realized',\n",
              " 'that',\n",
              " 'we',\n",
              " 'simply',\n",
              " 'could',\n",
              " 'not',\n",
              " 'find',\n",
              " 'it',\n",
              " 'anywhere',\n",
              " 'in',\n",
              " 'our',\n",
              " 'city',\n",
              " 'we',\n",
              " 'were',\n",
              " 'bummed.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " 'Now',\n",
              " ',',\n",
              " 'because',\n",
              " 'of',\n",
              " 'the',\n",
              " 'magic',\n",
              " 'of',\n",
              " 'the',\n",
              " 'internet',\n",
              " ',',\n",
              " 'we',\n",
              " 'have',\n",
              " 'a',\n",
              " 'case',\n",
              " 'of',\n",
              " 'the',\n",
              " 'sauce',\n",
              " 'and',\n",
              " 'are',\n",
              " 'ecstatic',\n",
              " 'because',\n",
              " 'of',\n",
              " 'it.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " 'If',\n",
              " 'you',\n",
              " 'love',\n",
              " 'hot',\n",
              " 'sauce..I',\n",
              " 'mean',\n",
              " 'really',\n",
              " 'love',\n",
              " 'hot',\n",
              " 'sauce',\n",
              " ',',\n",
              " 'but',\n",
              " 'do',\n",
              " 'not',\n",
              " 'want',\n",
              " 'a',\n",
              " 'sauce',\n",
              " 'that',\n",
              " 'tastelessly',\n",
              " 'burns',\n",
              " 'your',\n",
              " 'throat',\n",
              " ',',\n",
              " 'grab',\n",
              " 'a',\n",
              " 'bottle',\n",
              " 'of',\n",
              " 'Tequila',\n",
              " 'Picante',\n",
              " 'Gourmet',\n",
              " 'de',\n",
              " 'Inclan',\n",
              " '.',\n",
              " 'Just',\n",
              " 'realize',\n",
              " 'that',\n",
              " 'once',\n",
              " 'you',\n",
              " 'taste',\n",
              " 'it',\n",
              " ',',\n",
              " 'you',\n",
              " 'will',\n",
              " 'never',\n",
              " 'want',\n",
              " 'to',\n",
              " 'use',\n",
              " 'any',\n",
              " 'other',\n",
              " 'sauce.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " 'Thank',\n",
              " 'you',\n",
              " 'for',\n",
              " 'the',\n",
              " 'personal',\n",
              " ',',\n",
              " 'incredible',\n",
              " 'service',\n",
              " '!',\n",
              " '``',\n",
              " ',',\n",
              " '``',\n",
              " 'Product',\n",
              " 'received',\n",
              " 'is',\n",
              " 'as',\n",
              " 'advertised.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'a',\n",
              " \"href='http\",\n",
              " ':',\n",
              " '//www.amazon.com/gp/product/B001GVISJM',\n",
              " \"'\",\n",
              " '>',\n",
              " 'Twizzlers',\n",
              " ',',\n",
              " 'Strawberry',\n",
              " ',',\n",
              " '16-Ounce',\n",
              " 'Bags',\n",
              " '(',\n",
              " 'Pack',\n",
              " 'of',\n",
              " '6',\n",
              " ')',\n",
              " '<',\n",
              " '/a',\n",
              " '>',\n",
              " \"''\",\n",
              " ',',\n",
              " \"'this\",\n",
              " 'was',\n",
              " 'sooooo',\n",
              " 'deliscious',\n",
              " 'but',\n",
              " 'too',\n",
              " 'bad',\n",
              " 'i',\n",
              " 'ate',\n",
              " 'them',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'and',\n",
              " 'gained',\n",
              " '2',\n",
              " 'pds',\n",
              " '!',\n",
              " 'my',\n",
              " 'fault',\n",
              " \"'\",\n",
              " ',',\n",
              " '``',\n",
              " 'Deal',\n",
              " 'was',\n",
              " 'awesome',\n",
              " '!',\n",
              " 'Arrived',\n",
              " 'before',\n",
              " 'Halloween',\n",
              " 'as',\n",
              " 'indicated',\n",
              " 'and',\n",
              " 'was',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'satisfy',\n",
              " 'trick',\n",
              " 'or',\n",
              " 'treaters',\n",
              " '.',\n",
              " 'I',\n",
              " 'love',\n",
              " 'the',\n",
              " 'quality',\n",
              " 'of',\n",
              " 'this',\n",
              " 'product',\n",
              " 'and',\n",
              " 'it',\n",
              " 'was',\n",
              " 'much',\n",
              " 'less',\n",
              " 'expensive',\n",
              " 'than',\n",
              " 'the',\n",
              " 'local',\n",
              " 'store',\n",
              " \"'s\",\n",
              " 'candy',\n",
              " '.',\n",
              " '``',\n",
              " ',',\n",
              " \"'I\",\n",
              " 'love',\n",
              " 'these',\n",
              " '...',\n",
              " '...',\n",
              " '...',\n",
              " 'very',\n",
              " 'tasty',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " 'Infact',\n",
              " ',',\n",
              " 'I',\n",
              " 'think',\n",
              " 'I',\n",
              " 'am',\n",
              " 'addicted',\n",
              " 'to',\n",
              " 'them.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " 'Buying',\n",
              " 'them',\n",
              " 'in',\n",
              " 'packs',\n",
              " 'of',\n",
              " '6',\n",
              " 'bags',\n",
              " '-',\n",
              " 'is',\n",
              " 'very',\n",
              " 'reasonable',\n",
              " 'than',\n",
              " 'going',\n",
              " 'to',\n",
              " 'Target',\n",
              " 'and',\n",
              " 'getting',\n",
              " 'a',\n",
              " 'bag',\n",
              " '.',\n",
              " 'Savings',\n",
              " 'are',\n",
              " 'about',\n",
              " 'a',\n",
              " '$',\n",
              " '1.00',\n",
              " 'a',\n",
              " 'bag',\n",
              " '.',\n",
              " 'I',\n",
              " 'use',\n",
              " 'subscribe',\n",
              " 'and',\n",
              " 'save',\n",
              " 'on',\n",
              " 'these',\n",
              " 'and',\n",
              " 'several',\n",
              " 'other',\n",
              " 'product',\n",
              " '.',\n",
              " 'I',\n",
              " 'love',\n",
              " 'subscribe',\n",
              " 'and',\n",
              " 'save',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " \"'\",\n",
              " ',',\n",
              " '``',\n",
              " 'I',\n",
              " 'LOVE',\n",
              " 'spicy',\n",
              " 'ramen',\n",
              " ',',\n",
              " 'but',\n",
              " 'for',\n",
              " 'whatever',\n",
              " 'reasons',\n",
              " 'this',\n",
              " 'thing',\n",
              " 'burns',\n",
              " 'my',\n",
              " 'stomach',\n",
              " 'badly',\n",
              " 'and',\n",
              " 'the',\n",
              " 'burning',\n",
              " 'sensation',\n",
              " 'does',\n",
              " 'not',\n",
              " 'go',\n",
              " 'away',\n",
              " 'for',\n",
              " 'like',\n",
              " '3',\n",
              " 'hours',\n",
              " '!',\n",
              " 'Not',\n",
              " 'sure',\n",
              " 'if',\n",
              " 'that',\n",
              " 'is',\n",
              " 'healthy',\n",
              " 'or',\n",
              " 'not',\n",
              " '...',\n",
              " '.',\n",
              " 'and',\n",
              " 'you',\n",
              " 'can',\n",
              " 'buy',\n",
              " 'this',\n",
              " 'at',\n",
              " 'Walmart',\n",
              " 'for',\n",
              " '$',\n",
              " '0.28',\n",
              " ',',\n",
              " 'way',\n",
              " 'cheaper',\n",
              " 'than',\n",
              " 'Amazon',\n",
              " '.',\n",
              " '``',\n",
              " ',',\n",
              " \"'Makes\",\n",
              " 'a',\n",
              " 'tasty',\n",
              " ',',\n",
              " 'super',\n",
              " 'easy',\n",
              " 'meal',\n",
              " ',',\n",
              " 'fast',\n",
              " '.',\n",
              " 'BUT',\n",
              " 'high',\n",
              " 'in',\n",
              " 'calories.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " 'The',\n",
              " 'instructions',\n",
              " 'say',\n",
              " 'to',\n",
              " 'saute',\n",
              " 'the',\n",
              " 'veggies',\n",
              " 'first',\n",
              " 'but',\n",
              " 'I',\n",
              " 'recommend',\n",
              " 'cooking',\n",
              " 'the',\n",
              " 'chicken',\n",
              " 'first',\n",
              " '.',\n",
              " 'The',\n",
              " 'chicken',\n",
              " 'takes',\n",
              " 'longer',\n",
              " 'to',\n",
              " 'cook',\n",
              " 'and',\n",
              " 'the',\n",
              " 'raw',\n",
              " 'chicken',\n",
              " 'ontop',\n",
              " 'of',\n",
              " 'veggies',\n",
              " 'just',\n",
              " 'makes',\n",
              " 'a',\n",
              " 'slimy',\n",
              " 'mess',\n",
              " '.',\n",
              " 'I',\n",
              " 'made',\n",
              " 'it',\n",
              " 'with',\n",
              " 'snow',\n",
              " 'peas',\n",
              " 'and',\n",
              " 'carrots',\n",
              " 'only',\n",
              " '.',\n",
              " 'I',\n",
              " 'do',\n",
              " 'not',\n",
              " 'like',\n",
              " 'the',\n",
              " 'little',\n",
              " 'corn',\n",
              " '.',\n",
              " 'Added',\n",
              " 'some',\n",
              " 'red',\n",
              " 'pepper',\n",
              " 'flakes',\n",
              " 'for',\n",
              " 'heat',\n",
              " 'and',\n",
              " 'served',\n",
              " 'ontop',\n",
              " 'of',\n",
              " 'rice',\n",
              " '.',\n",
              " 'It',\n",
              " 'came',\n",
              " 'out',\n",
              " 'wonderful',\n",
              " '!',\n",
              " 'Dinner',\n",
              " 'on',\n",
              " 'the',\n",
              " 'table',\n",
              " 'in',\n",
              " 'less',\n",
              " 'than',\n",
              " '30mins',\n",
              " '.',\n",
              " \"'\",\n",
              " ',',\n",
              " \"'Love\",\n",
              " 'this',\n",
              " 'sugar',\n",
              " '.',\n",
              " 'I',\n",
              " 'also',\n",
              " 'get',\n",
              " 'muscavado',\n",
              " 'sugar',\n",
              " 'and',\n",
              " 'they',\n",
              " 'are',\n",
              " 'both',\n",
              " 'great',\n",
              " 'to',\n",
              " 'use',\n",
              " 'in',\n",
              " 'place',\n",
              " 'of',\n",
              " 'regular',\n",
              " 'white',\n",
              " 'sugar',\n",
              " '.',\n",
              " 'Recommend',\n",
              " '!',\n",
              " \"'\",\n",
              " ',',\n",
              " '``',\n",
              " 'This',\n",
              " 'is',\n",
              " 'just',\n",
              " 'Fantastic',\n",
              " 'Chicken',\n",
              " 'Noodle',\n",
              " 'soup',\n",
              " ',',\n",
              " 'the',\n",
              " 'best',\n",
              " 'I',\n",
              " 'have',\n",
              " 'ever',\n",
              " 'eaten',\n",
              " ',',\n",
              " 'with',\n",
              " 'large',\n",
              " 'hearty',\n",
              " 'chunks',\n",
              " 'of',\n",
              " 'chicken',\n",
              " ',',\n",
              " 'and',\n",
              " 'vegetables',\n",
              " 'and',\n",
              " 'nice',\n",
              " 'large',\n",
              " 'noodles',\n",
              " '.',\n",
              " 'This',\n",
              " 'soup',\n",
              " 'is',\n",
              " 'just',\n",
              " 'so',\n",
              " 'full',\n",
              " 'bodied',\n",
              " ',',\n",
              " 'and',\n",
              " 'is',\n",
              " 'seasoned',\n",
              " 'just',\n",
              " 'right',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'so',\n",
              " 'glad',\n",
              " 'Amazon',\n",
              " 'carries',\n",
              " 'this',\n",
              " 'product',\n",
              " '.',\n",
              " 'I',\n",
              " 'just',\n",
              " 'can',\n",
              " 'not',\n",
              " 'find',\n",
              " 'it',\n",
              " 'here',\n",
              " 'in',\n",
              " 'Vermont',\n",
              " '.',\n",
              " \"''\",\n",
              " ']']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A6t9QRJ1enMC",
        "outputId": "e8019987-5d99-4318-d6ab-eb38846ce6ec"
      },
      "source": [
        "#stopword removal\n",
        "def remove_stopwords(words):\n",
        "#\"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "d = remove_stopwords(tokenization)\n",
        "remstopword=' '.join(d)\n",
        "remstopword\n",
        "#不知道在jupyter notebook上面跑得起來但在colab上有時候跑得起來有時候不行..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-43c3718652a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mnew_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mremstopword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mremstopword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-43c3718652a2>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnew_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mnew_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "leDCqIA8enMD",
        "outputId": "df883901-3b7a-49ca-8c8f-1883810247b6"
      },
      "source": [
        "# lemmatize verbs\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    dick = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in str(words).split():\n",
        "        # POS 要指定'v'，否則詞性不會變，因為詞型的 default POS 是'n'\n",
        "        # POS = Part-Of-Speech\n",
        "        lemma = dick.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "f=lemmatize_verbs(remstopword)\n",
        "lemmatize=' '.join(f)\n",
        "lemmatize\n",
        "#source:https://www.jianshu.com/p/79255fe0c5b5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7ee3be87182a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlemmas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlemmatize_verbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremstopword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'remstopword' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wLKfaWcenME",
        "outputId": "00055dde-694a-40f4-ee8c-e3919afcf48d"
      },
      "source": [
        "#stemming \n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer()  \n",
        "words = word_tokenize(lemmatize) \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) \n",
        "#source:https://www.geeksforgeeks.org/python-stemming-words-with-nltk/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  :  [\n",
            "``  :  ``\n",
            "Product  :  product\n",
            "arrive  :  arriv\n",
            "label  :  label\n",
            "Jumbo  :  jumbo\n",
            "Salted  :  salt\n",
            "Peanuts  :  peanut\n",
            "...  :  ...\n",
            "peanuts  :  peanut\n",
            "actually  :  actual\n",
            "small  :  small\n",
            "size  :  size\n",
            "unsalted  :  unsalt\n",
            ".  :  .\n",
            "Not  :  not\n",
            "sure  :  sure\n",
            "error  :  error\n",
            "vendor  :  vendor\n",
            "intend  :  intend\n",
            "represent  :  repres\n",
            "product  :  product\n",
            "'Jumbo  :  'jumbo\n",
            "'  :  '\n",
            "``  :  ``\n",
            ",  :  ,\n",
            "'  :  '\n",
            "I  :  I\n",
            "WAS  :  wa\n",
            "VISITING  :  visit\n",
            "MY  :  MY\n",
            "FRIEND  :  friend\n",
            "NATE  :  nate\n",
            "THE  :  the\n",
            "OTHER  :  other\n",
            "MORNING  :  morn\n",
            "FOR  :  for\n",
            "COFFEE  :  coffe\n",
            ",  :  ,\n",
            "HE  :  HE\n",
            "CAME  :  came\n",
            "OUT  :  out\n",
            "OF  :  OF\n",
            "HIS  :  hi\n",
            "STORAGE  :  storag\n",
            "ROOM  :  room\n",
            "WITH  :  with\n",
            "(  :  (\n",
            "A  :  A\n",
            "PACKET  :  packet\n",
            "OF  :  OF\n",
            "McCANNS  :  mccann\n",
            "INSTANT  :  instant\n",
            "IRISH  :  irish\n",
            "OATMEAL  :  oatmeal\n",
            ".  :  .\n",
            ")  :  )\n",
            "HE  :  HE\n",
            "SUGGESTED  :  suggest\n",
            "THAT  :  that\n",
            "I  :  I\n",
            "TRY  :  tri\n",
            "IT  :  IT\n",
            "FOR  :  for\n",
            "MY  :  MY\n",
            "OWN  :  own\n",
            "USE  :  use\n",
            ",  :  ,\n",
            "IN  :  IN\n",
            "MY  :  MY\n",
            "STASH  :  stash\n",
            ".  :  .\n",
            "SOMETIMES  :  sometim\n",
            "NATE  :  nate\n",
            "DOSE  :  dose\n",
            "NOT  :  not\n",
            "GIVE  :  give\n",
            "YOU  :  you\n",
            "A  :  A\n",
            "CHANCE  :  chanc\n",
            "TO  :  TO\n",
            "SAY  :  say\n",
            "NO  :  NO\n",
            ",  :  ,\n",
            "SO  :  SO\n",
            "I  :  I\n",
            "ENDED  :  end\n",
            "UP  :  UP\n",
            "TRYING  :  tri\n",
            "THE  :  the\n",
            "APPLE  :  appl\n",
            "AND  :  and\n",
            "CINN  :  cinn\n",
            ".  :  .\n",
            "FOUND  :  found\n",
            "IT  :  IT\n",
            "TO  :  TO\n",
            "BE  :  BE\n",
            "VERY  :  veri\n",
            "TASTEFULL  :  tasteful\n",
            "WHEN  :  when\n",
            "MADE  :  made\n",
            "WITH  :  with\n",
            "WATER  :  water\n",
            "OR  :  OR\n",
            "POWDERED  :  powder\n",
            "MILK  :  milk\n",
            ".  :  .\n",
            "IT  :  IT\n",
            "GOES  :  goe\n",
            "GOOD  :  good\n",
            "WITH  :  with\n",
            "of.J  :  of.j\n",
            ".  :  .\n",
            "AND  :  and\n",
            "COFFEE  :  coffe\n",
            "AND  :  and\n",
            "A  :  A\n",
            "SLICE  :  slice\n",
            "OF  :  OF\n",
            "TOAST  :  toast\n",
            "AND  :  and\n",
            "YOUR  :  your\n",
            "READY  :  readi\n",
            "TO  :  TO\n",
            "TAKE  :  take\n",
            "ON  :  ON\n",
            "THE  :  the\n",
            "WORLD  :  world\n",
            "...  :  ...\n",
            "OR  :  OR\n",
            "THE  :  the\n",
            "DAY  :  day\n",
            "AT  :  AT\n",
            "LEAST  :  least\n",
            "..  :  ..\n",
            "JERRY  :  jerri\n",
            "REITH  :  reith\n",
            "...  :  ...\n",
            "'  :  '\n",
            ",  :  ,\n",
            "``  :  ``\n",
            "I  :  I\n",
            "know  :  know\n",
            "cactus  :  cactu\n",
            "tequila  :  tequila\n",
            "unique  :  uniqu\n",
            "combination  :  combin\n",
            "ingredients  :  ingredi\n",
            ",  :  ,\n",
            "flavour  :  flavour\n",
            "hot  :  hot\n",
            "sauce  :  sauc\n",
            "make  :  make\n",
            "one  :  one\n",
            "kind  :  kind\n",
            "!  :  !\n",
            "We  :  We\n",
            "pick  :  pick\n",
            "bottle  :  bottl\n",
            "trip  :  trip\n",
            "bring  :  bring\n",
            "back  :  back\n",
            "home  :  home\n",
            "us  :  us\n",
            "totally  :  total\n",
            "blow  :  blow\n",
            "away  :  away\n",
            "!  :  !\n",
            "When  :  when\n",
            "realize  :  realiz\n",
            "simply  :  simpli\n",
            "could  :  could\n",
            "find  :  find\n",
            "anywhere  :  anywher\n",
            "city  :  citi\n",
            "bummed  :  bum\n",
            ".  :  .\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "Now  :  now\n",
            ",  :  ,\n",
            "magic  :  magic\n",
            "internet  :  internet\n",
            ",  :  ,\n",
            "case  :  case\n",
            "sauce  :  sauc\n",
            "ecstatic  :  ecstat\n",
            "it  :  it\n",
            ".  :  .\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "If  :  If\n",
            "love  :  love\n",
            "hot  :  hot\n",
            "sauce  :  sauc\n",
            "..  :  ..\n",
            "I  :  I\n",
            "mean  :  mean\n",
            "really  :  realli\n",
            "love  :  love\n",
            "hot  :  hot\n",
            "sauce  :  sauc\n",
            ",  :  ,\n",
            "want  :  want\n",
            "sauce  :  sauc\n",
            "tastelessly  :  tastelessli\n",
            "burn  :  burn\n",
            "throat  :  throat\n",
            ",  :  ,\n",
            "grab  :  grab\n",
            "bottle  :  bottl\n",
            "Tequila  :  tequila\n",
            "Picante  :  picant\n",
            "Gourmet  :  gourmet\n",
            "de  :  de\n",
            "Inclan  :  inclan\n",
            ".  :  .\n",
            "Just  :  just\n",
            "realize  :  realiz\n",
            "taste  :  tast\n",
            ",  :  ,\n",
            "never  :  never\n",
            "want  :  want\n",
            "use  :  use\n",
            "sauce  :  sauc\n",
            ".  :  .\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "Thank  :  thank\n",
            "personal  :  person\n",
            ",  :  ,\n",
            "incredible  :  incred\n",
            "service  :  servic\n",
            "!  :  !\n",
            "``  :  ``\n",
            ",  :  ,\n",
            "``  :  ``\n",
            "Product  :  product\n",
            "receive  :  receiv\n",
            "advertised  :  advertis\n",
            ".  :  .\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "<  :  <\n",
            "href='http  :  href='http\n",
            ":  :  :\n",
            "//www.amazon.com/gp/product/B001GVISJM  :  //www.amazon.com/gp/product/b001gvisjm\n",
            "'  :  '\n",
            ">  :  >\n",
            "Twizzlers  :  twizzler\n",
            ",  :  ,\n",
            "Strawberry  :  strawberri\n",
            ",  :  ,\n",
            "16-Ounce  :  16-ounc\n",
            "Bags  :  bag\n",
            "(  :  (\n",
            "Pack  :  pack\n",
            "6  :  6\n",
            ")  :  )\n",
            "<  :  <\n",
            "/a  :  /a\n",
            ">  :  >\n",
            "``  :  ``\n",
            ",  :  ,\n",
            "'this  :  'thi\n",
            "sooooo  :  sooooo\n",
            "deliscious  :  delisci\n",
            "bad  :  bad\n",
            "eat  :  eat\n",
            "fast  :  fast\n",
            "gain  :  gain\n",
            "2  :  2\n",
            "pds  :  pd\n",
            "!  :  !\n",
            "fault  :  fault\n",
            "'  :  '\n",
            ",  :  ,\n",
            "``  :  ``\n",
            "Deal  :  deal\n",
            "awesome  :  awesom\n",
            "!  :  !\n",
            "Arrived  :  arriv\n",
            "Halloween  :  halloween\n",
            "indicate  :  indic\n",
            "enough  :  enough\n",
            "satisfy  :  satisfi\n",
            "trick  :  trick\n",
            "treaters  :  treater\n",
            ".  :  .\n",
            "I  :  I\n",
            "love  :  love\n",
            "quality  :  qualiti\n",
            "product  :  product\n",
            "much  :  much\n",
            "less  :  less\n",
            "expensive  :  expens\n",
            "local  :  local\n",
            "store  :  store\n",
            "'s  :  's\n",
            "candy  :  candi\n",
            ".  :  .\n",
            "``  :  ``\n",
            ",  :  ,\n",
            "'  :  '\n",
            "I  :  I\n",
            "love  :  love\n",
            ".........  :  .........\n",
            "tasty  :  tasti\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "Infact  :  infact\n",
            ",  :  ,\n",
            "I  :  I\n",
            "think  :  think\n",
            "I  :  I\n",
            "addict  :  addict\n",
            "them  :  them\n",
            ".  :  .\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "Buying  :  buy\n",
            "pack  :  pack\n",
            "6  :  6\n",
            "bag  :  bag\n",
            "-  :  -\n",
            "reasonable  :  reason\n",
            "go  :  go\n",
            "Target  :  target\n",
            "get  :  get\n",
            "bag  :  bag\n",
            ".  :  .\n",
            "Savings  :  save\n",
            "$  :  $\n",
            "1.00  :  1.00\n",
            "bag  :  bag\n",
            ".  :  .\n",
            "I  :  I\n",
            "use  :  use\n",
            "subscribe  :  subscrib\n",
            "save  :  save\n",
            "several  :  sever\n",
            "product  :  product\n",
            ".  :  .\n",
            "I  :  I\n",
            "love  :  love\n",
            "subscribe  :  subscrib\n",
            "save  :  save\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "!  :  !\n",
            "'  :  '\n",
            ",  :  ,\n",
            "``  :  ``\n",
            "I  :  I\n",
            "LOVE  :  love\n",
            "spicy  :  spici\n",
            "ramen  :  ramen\n",
            ",  :  ,\n",
            "whatever  :  whatev\n",
            "reason  :  reason\n",
            "thing  :  thing\n",
            "burn  :  burn\n",
            "stomach  :  stomach\n",
            "badly  :  badli\n",
            "burn  :  burn\n",
            "sensation  :  sensat\n",
            "go  :  go\n",
            "away  :  away\n",
            "like  :  like\n",
            "3  :  3\n",
            "hours  :  hour\n",
            "!  :  !\n",
            "Not  :  not\n",
            "sure  :  sure\n",
            "healthy  :  healthi\n",
            "....  :  ....\n",
            "buy  :  buy\n",
            "Walmart  :  walmart\n",
            "$  :  $\n",
            "0.28  :  0.28\n",
            ",  :  ,\n",
            "way  :  way\n",
            "cheaper  :  cheaper\n",
            "Amazon  :  amazon\n",
            ".  :  .\n",
            "``  :  ``\n",
            ",  :  ,\n",
            "'Makes  :  'make\n",
            "tasty  :  tasti\n",
            ",  :  ,\n",
            "super  :  super\n",
            "easy  :  easi\n",
            "meal  :  meal\n",
            ",  :  ,\n",
            "fast  :  fast\n",
            ".  :  .\n",
            "BUT  :  but\n",
            "high  :  high\n",
            "calories  :  calori\n",
            ".  :  .\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "<  :  <\n",
            "br  :  br\n",
            "/  :  /\n",
            ">  :  >\n",
            "The  :  the\n",
            "instructions  :  instruct\n",
            "say  :  say\n",
            "saute  :  saut\n",
            "veggies  :  veggi\n",
            "first  :  first\n",
            "I  :  I\n",
            "recommend  :  recommend\n",
            "cook  :  cook\n",
            "chicken  :  chicken\n",
            "first  :  first\n",
            ".  :  .\n",
            "The  :  the\n",
            "chicken  :  chicken\n",
            "take  :  take\n",
            "longer  :  longer\n",
            "cook  :  cook\n",
            "raw  :  raw\n",
            "chicken  :  chicken\n",
            "ontop  :  ontop\n",
            "veggies  :  veggi\n",
            "make  :  make\n",
            "slimy  :  slimi\n",
            "mess  :  mess\n",
            ".  :  .\n",
            "I  :  I\n",
            "make  :  make\n",
            "snow  :  snow\n",
            "peas  :  pea\n",
            "carrots  :  carrot\n",
            ".  :  .\n",
            "I  :  I\n",
            "like  :  like\n",
            "little  :  littl\n",
            "corn  :  corn\n",
            ".  :  .\n",
            "Added  :  ad\n",
            "red  :  red\n",
            "pepper  :  pepper\n",
            "flake  :  flake\n",
            "heat  :  heat\n",
            "serve  :  serv\n",
            "ontop  :  ontop\n",
            "rice  :  rice\n",
            ".  :  .\n",
            "It  :  It\n",
            "come  :  come\n",
            "wonderful  :  wonder\n",
            "!  :  !\n",
            "Dinner  :  dinner\n",
            "table  :  tabl\n",
            "less  :  less\n",
            "30mins  :  30min\n",
            ".  :  .\n",
            "'  :  '\n",
            ",  :  ,\n",
            "'Love  :  'love\n",
            "sugar  :  sugar\n",
            ".  :  .\n",
            "I  :  I\n",
            "also  :  also\n",
            "get  :  get\n",
            "muscavado  :  muscavado\n",
            "sugar  :  sugar\n",
            "great  :  great\n",
            "use  :  use\n",
            "place  :  place\n",
            "regular  :  regular\n",
            "white  :  white\n",
            "sugar  :  sugar\n",
            ".  :  .\n",
            "Recommend  :  recommend\n",
            "!  :  !\n",
            "'  :  '\n",
            ",  :  ,\n",
            "``  :  ``\n",
            "This  :  thi\n",
            "Fantastic  :  fantast\n",
            "Chicken  :  chicken\n",
            "Noodle  :  noodl\n",
            "soup  :  soup\n",
            ",  :  ,\n",
            "best  :  best\n",
            "I  :  I\n",
            "ever  :  ever\n",
            "eat  :  eat\n",
            ",  :  ,\n",
            "large  :  larg\n",
            "hearty  :  hearti\n",
            "chunk  :  chunk\n",
            "chicken  :  chicken\n",
            ",  :  ,\n",
            "vegetables  :  veget\n",
            "nice  :  nice\n",
            "large  :  larg\n",
            "noodles  :  noodl\n",
            ".  :  .\n",
            "This  :  thi\n",
            "soup  :  soup\n",
            "full  :  full\n",
            "body  :  bodi\n",
            ",  :  ,\n",
            "season  :  season\n",
            "right  :  right\n",
            ".  :  .\n",
            "I  :  I\n",
            "glad  :  glad\n",
            "Amazon  :  amazon\n",
            "carry  :  carri\n",
            "product  :  product\n",
            ".  :  .\n",
            "I  :  I\n",
            "find  :  find\n",
            "Vermont  :  vermont\n",
            ".  :  .\n",
            "``  :  ``\n",
            "]  :  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a8slpNZh_ID"
      },
      "source": [
        "# Question 2: DataFrame handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGkOfX9LiF4o"
      },
      "source": [
        "\n",
        "\n",
        "*   Please download the datasets from the following link, https://www.kaggle.com/aaron7sun/stocknews\n",
        "*   Save the downloaded files on your own drive and load the file for later use.\n",
        "- There are two channels of data provided in this dataset:\n",
        "\n",
        "  - **News data:** Crawled historical news headlines from Reddit WorldNews Channel . They are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)\n",
        "\n",
        "  - **Stock data:** Dow Jones Industrial Average (DJIA) is used to \"prove the concept\". (Range: 2008-08-08 to 2016-07-01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaguSDD3iO_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3914bca9-c0d3-44ab-8b5e-633c52f19349"
      },
      "source": [
        "!pip install --upgrade google-api-python-client\n",
        "#source:https://stackoverflow.com/questions/36183486/importerror-no-module-named-google\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g6cwQZcFvmKs_er9M81KN3gVEVzRDIsxAaDMJTaXdbIOuob4iX1g7A\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX970ixtiPnh"
      },
      "source": [
        "import pandas as pd\n",
        "news_df = pd.read_csv('/content/drive/MyDrive/RedditNews.csv',engine='python',sep='delimiter', header=None,error_bad_lines=False)\n",
        "stock_df = pd.read_csv('/content/drive/MyDrive/upload_DJIA_table.csv',engine='python',sep='delimiter', header=None,error_bad_lines=False)\n",
        "#https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMR3Rv5iRV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "bc0e80fa-23af-4441-c599-8f43c5f0270f"
      },
      "source": [
        "stock_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Date,Open,High,Low,Close,Volume,Adj Close</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01,17924.240234,18002.380859,17916.910...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-06-30,17712.759766,17930.609375,17711.800...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-06-29,17456.019531,17704.509766,17456.019...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-06-28,17190.509766,17409.720703,17190.509...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>2008-08-14,11532.070312,11718.280273,11450.889...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1986</th>\n",
              "      <td>2008-08-13,11632.80957,11633.780273,11453.3398...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>2008-08-12,11781.700195,11782.349609,11601.519...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>2008-08-11,11729.669922,11867.110352,11675.530...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>2008-08-08,11432.089844,11759.959961,11388.040...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1990 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0\n",
              "0             Date,Open,High,Low,Close,Volume,Adj Close\n",
              "1     2016-07-01,17924.240234,18002.380859,17916.910...\n",
              "2     2016-06-30,17712.759766,17930.609375,17711.800...\n",
              "3     2016-06-29,17456.019531,17704.509766,17456.019...\n",
              "4     2016-06-28,17190.509766,17409.720703,17190.509...\n",
              "...                                                 ...\n",
              "1985  2008-08-14,11532.070312,11718.280273,11450.889...\n",
              "1986  2008-08-13,11632.80957,11633.780273,11453.3398...\n",
              "1987  2008-08-12,11781.700195,11782.349609,11601.519...\n",
              "1988  2008-08-11,11729.669922,11867.110352,11675.530...\n",
              "1989  2008-08-08,11432.089844,11759.959961,11388.040...\n",
              "\n",
              "[1990 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "KFqPUNxWkK-K",
        "outputId": "6ecba76d-6639-4ef8-e005-677faa15e077"
      },
      "source": [
        "news_df.head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Date,News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01,\"A 117-year-old woman in Mexico Cit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-01,IMF chief backs Athens as permanent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-01,\"The president of France says if Br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-01,British Man Who Must Give Police 24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2016-07-01,100+ Nobel laureates urge Greenpeac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016-07-01,Brazil: Huge spike in number of pol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2016-07-01,Austria's highest court annuls pres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2016-07-01,\"Facebook wins privacy case, can tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2016-07-01,\"Switzerland denies Muslim girls ci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2016-07-01,\"China kills millions of innocent m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2016-07-01,\"France Cracks Down on Factory Farm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2016-07-01,Abbas PLO Faction Calls Killer of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2016-07-01,Taiwanese warship accidentally fire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2016-07-01,\"Iran celebrates American Human Rig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0\n",
              "0                                           Date,News\n",
              "1   2016-07-01,\"A 117-year-old woman in Mexico Cit...\n",
              "2   2016-07-01,IMF chief backs Athens as permanent...\n",
              "3   2016-07-01,\"The president of France says if Br...\n",
              "4   2016-07-01,British Man Who Must Give Police 24...\n",
              "5   2016-07-01,100+ Nobel laureates urge Greenpeac...\n",
              "6   2016-07-01,Brazil: Huge spike in number of pol...\n",
              "7   2016-07-01,Austria's highest court annuls pres...\n",
              "8   2016-07-01,\"Facebook wins privacy case, can tr...\n",
              "9   2016-07-01,\"Switzerland denies Muslim girls ci...\n",
              "10  2016-07-01,\"China kills millions of innocent m...\n",
              "11  2016-07-01,\"France Cracks Down on Factory Farm...\n",
              "12  2016-07-01,Abbas PLO Faction Calls Killer of 1...\n",
              "13  2016-07-01,Taiwanese warship accidentally fire...\n",
              "14  2016-07-01,\"Iran celebrates American Human Rig..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "smpTiq1DiST4",
        "outputId": "eaaaaac3-b4b0-41b6-beca-4826c46c811b"
      },
      "source": [
        "stock_df.head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Date,Open,High,Low,Close,Volume,Adj Close</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01,17924.240234,18002.380859,17916.910...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-06-30,17712.759766,17930.609375,17711.800...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-06-29,17456.019531,17704.509766,17456.019...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-06-28,17190.509766,17409.720703,17190.509...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2016-06-27,17355.210938,17355.210938,17063.080...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016-06-24,17946.630859,17946.630859,17356.339...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2016-06-23,17844.109375,18011.070312,17844.109...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2016-06-22,17832.669922,17920.160156,17770.359...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2016-06-21,17827.330078,17877.839844,17799.800...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2016-06-20,17736.869141,17946.359375,17736.869...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2016-06-17,17733.439453,17733.439453,17602.779...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2016-06-16,17602.230469,17754.910156,17471.289...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2016-06-15,17703.650391,17762.960938,17629.009...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2016-06-14,17710.769531,17733.919922,17595.789...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0\n",
              "0           Date,Open,High,Low,Close,Volume,Adj Close\n",
              "1   2016-07-01,17924.240234,18002.380859,17916.910...\n",
              "2   2016-06-30,17712.759766,17930.609375,17711.800...\n",
              "3   2016-06-29,17456.019531,17704.509766,17456.019...\n",
              "4   2016-06-28,17190.509766,17409.720703,17190.509...\n",
              "5   2016-06-27,17355.210938,17355.210938,17063.080...\n",
              "6   2016-06-24,17946.630859,17946.630859,17356.339...\n",
              "7   2016-06-23,17844.109375,18011.070312,17844.109...\n",
              "8   2016-06-22,17832.669922,17920.160156,17770.359...\n",
              "9   2016-06-21,17827.330078,17877.839844,17799.800...\n",
              "10  2016-06-20,17736.869141,17946.359375,17736.869...\n",
              "11  2016-06-17,17733.439453,17733.439453,17602.779...\n",
              "12  2016-06-16,17602.230469,17754.910156,17471.289...\n",
              "13  2016-06-15,17703.650391,17762.960938,17629.009...\n",
              "14  2016-06-14,17710.769531,17733.919922,17595.789..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H03CMsosjphW"
      },
      "source": [
        "\n",
        "## 2-1. Define a *funtion* aims to know the weekly (7 days) Stock **Close value** trend. (80%)\n",
        "- Implement a *funtion* that determines the weekly (7 days) Stock **Close value** trend using the Stock data and **then record the trend in the News data in a new column \"label\" with 0, 1 and -1**. For example, \n",
        "  - On 2016-06-13, the market closed at 17732.480469. Seven days later, 2016-06-20, the market closed **higher** at 17804.869141. In this scenario, all entries corresponding to 2016-06-13 in the News data will be **marked 1** in the \"label\" column. Dates 2016-06-14 and 2016-06-15 will also be marked 1 because 2016-06-21 and 2016-06-22 closed higher, respectively.\n",
        "  - On the other hand, 2016-06-17 will be **marked 0** because 2016-06-24 (7 days later) closed **lower**.\n",
        "  - If a given date does not have a corresponding date for 7 days later, the given date will be **marked -1**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yf_CKVXkNy8"
      },
      "source": [
        "import datetime\n",
        "def label():\n",
        "  '''\n",
        "  please answer here, you can add any parameters if you want.\n",
        "  but don't import other libraries, this notebook already prepared the libraries which all you need !\n",
        "  remember that, the standard for evaluation include your:\n",
        "    1. Time Complexity (80%)\n",
        "    2. Program Logic (10%)\n",
        "    3. Creativity (10%)\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUM9RQjkkOIk",
        "outputId": "3593d670-da94-4bf9-9450-4086b31d6d8c"
      },
      "source": [
        "'''\n",
        "than apply your defined function on stock_df here.\n",
        "return results store into a new columns name \"Label\"\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nthan apply your defined function on stock_df here.\\nreturn results store into a new columns name \"Label\"\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wJuVxk2xWkL"
      },
      "source": [
        "## 2-2. Map your label to news data (20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djOnaxddkPFc"
      },
      "source": [
        "def news_label():\n",
        "  '''\n",
        "  Map your close value trend label in news data by date, also name it \"Label\"\n",
        "  if a date does not appear in the stock data, also label it as -1.\n",
        "  e.g. all news in 2016-06-30 will be label -1\n",
        "  '''\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1s56c_DiAhd"
      },
      "source": [
        "# Question 3: Compute cosine similarity of TF-IDF (term frequency–inverse document frequency)\n",
        "-  **Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. \n",
        "- **TF-IDF** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbAzTgLlqp0"
      },
      "source": [
        "## 3-1. Please answer why we can use cosine similarity to measure TF-IDF representation. Is there any other representation methods also can be measured by cosine similarity? (20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9qd7tYFenMK"
      },
      "source": [
        "TF-IDF是一種統計方法，用以評估一字詞對於一個檔案集或一個語料庫中的其中一份檔案的重要程度。\n",
        "tf是指某一個字詞在某一份文件中的出現頻率\n",
        "idf衡量該字詞的重要程度\n",
        "如果一個詞在一個文件中很長出現，但是在別的文件中非常少出現，那其實也就代表這個詞對這篇文章來說是重要的！\n",
        "字詞的重要性隨著它在檔案中出現的次數(TF)成正比增加，但同時會隨著它在語料庫中出現的頻率(IDF)成反比下降。\n",
        "所以如果在特定的檔案中的高tf的字詞及他在整個語料庫中的低出現頻率（idf)，就會產生高權重的tfidf!\n",
        "\n",
        "#source:https://github.com/Larix/TF-IDF_Tutorial\n",
        "#https://ithelp.ithome.com.tw/articles/10192323\n",
        "#https://zhuanlan.zhihu.com/p/32826433"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LPEFCfenML",
        "outputId": "7c8c6d30-f261-47ce-f620-f2212d637db4"
      },
      "source": [
        "餘弦相似度是透過測量兩個向量的夾角的餘弦值來測量它們之間的相似性。 \n",
        "0度角的餘弦值是1，而其他任何角度的餘弦值都不大於1，最小值是-1。 \n",
        "\n",
        "從兩個向量之間的角度的餘弦值確定兩個向量他們大概指向相同的方向。 \n",
        "\n",
        "如果兩個向量有相同的指向時，餘弦相似度的值就是1；\n",
        "\n",
        "如果兩個向量夾角為90°時，餘弦相似度的值為0；\n",
        "\n",
        "如果兩個向量指向完全相反的方向時，餘弦相似度的值為-1。 \n",
        "\n",
        "結果與向量的長度無關的，只跟向量的指向方向相關。 \n",
        "\n",
        "所以在例題裡面，每個詞項都會被賦予不同的維度，而一個文件由一個向量表示，其各個維度上的值=該詞項在文件中出現的頻率。 \n",
        "\n",
        "餘弦相似度就可以代表兩篇文件在主題上的相似度。 \n",
        "\n",
        "在信息檢索的情況下，由於一個詞的頻率（TF-IDF權）不能為負數，所以這兩個文件的餘弦相似性範圍從0到1。 \n",
        "\n",
        "所以\"找出相似文章\"的一種算法就是使用TF-IDF算法，找出兩篇文章的關鍵詞；\n",
        "\n",
        "1. 每篇文章各取出若干個關鍵詞（比如20個），合併成一個集合\n",
        "2. 計算每篇文章對於這個集合中的詞的詞頻（為了避免文章長度的差異，可以使用相對詞頻）\n",
        "3. 生成兩篇文章各自的詞頻向量；\n",
        "4. 計算兩個向量的餘弦相似度，值越大就表示越相似。 \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character in identifier (<ipython-input-144-54fbe371d2e5>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-144-54fbe371d2e5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    餘弦相似性通過測量兩個向量的夾角的餘弦值來度量它們之間的相似性。\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAx5GYvJlsLU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0FYrALsls2z"
      },
      "source": [
        "## 3-2. Define a converting function to compute tf-idf vector from a list of ducoments. (40%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtcko0fkTIz"
      },
      "source": [
        "documents = ['terrible service this time','terrible terrible service','most terrible service','terrible service and experience','what a terrible service','so terrible service experience','what a terrible disappointment','what a terrible place','this time it was so horrible','the staff was horrible']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAQN_egi4liK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747f3663-a3b7-4d0b-9227-fcd27de2094d"
      },
      "source": [
        "word_list = []\n",
        "for i in range(len(documents )):\n",
        "    word_list.append(documents [i].split(' '))\n",
        "print(word_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['terrible', 'service', 'this', 'time'], ['terrible', 'terrible', 'service'], ['most', 'terrible', 'service'], ['terrible', 'service', 'and', 'experience'], ['what', 'a', 'terrible', 'service'], ['so', 'terrible', 'service', 'experience'], ['what', 'a', 'terrible', 'disappointment'], ['what', 'a', 'terrible', 'place'], ['this', 'time', 'it', 'was', 'so', 'horrible'], ['the', 'staff', 'was', 'horrible']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr9SrY4A5UZ0",
        "outputId": "d0c2a608-96a5-4611-e85d-7c3d010a68df"
      },
      "source": [
        "# 使用Counter套件統計一段句子裏面所有字符出現次數\n",
        "import collections\n",
        "countlist = []\n",
        "for i in range(len(word_list)):\n",
        "    count = collections.Counter(word_list[i])\n",
        "    countlist.append(count)\n",
        "countlist\n",
        "#https://stackoverflow.com/questions/6400538/using-counter-in-python-3-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Counter({'service': 1, 'terrible': 1, 'this': 1, 'time': 1}),\n",
              " Counter({'service': 1, 'terrible': 2}),\n",
              " Counter({'most': 1, 'service': 1, 'terrible': 1}),\n",
              " Counter({'and': 1, 'experience': 1, 'service': 1, 'terrible': 1}),\n",
              " Counter({'a': 1, 'service': 1, 'terrible': 1, 'what': 1}),\n",
              " Counter({'experience': 1, 'service': 1, 'so': 1, 'terrible': 1}),\n",
              " Counter({'a': 1, 'disappointment': 1, 'terrible': 1, 'what': 1}),\n",
              " Counter({'a': 1, 'place': 1, 'terrible': 1, 'what': 1}),\n",
              " Counter({'horrible': 1, 'it': 1, 'so': 1, 'this': 1, 'time': 1, 'was': 1}),\n",
              " Counter({'horrible': 1, 'staff': 1, 'the': 1, 'was': 1})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyyhTBAs5us6"
      },
      "source": [
        "# word 可以通過count得到，count可以通過countlist\n",
        "# word[word] 可以得到每個單詞的詞頻， sum(count.values())得到整個句子的單詞總數\n",
        "def tf(word,count):\n",
        "    return count[word] / sum(count.values())\n",
        "\n",
        "# 統計的是含有該單詞的句子數\n",
        "def n_containing(word, count_list):\n",
        "    return sum(1 for count in count_list if word in count)\n",
        "\n",
        "# len(count_list)是指句子的總數， n_containing(word, count_list) 是指含有該單詞的句子總數， 加1是爲了防止分母爲0.\n",
        "def idf(word, count_list):\n",
        "    return math.log(len(count_list) / (1 + n_containing(word, count_list)))\n",
        "\n",
        "# 將tf和idf相乘\n",
        "def tfidf(word, count, count_list):\n",
        "    return tf(word,count) * idf(word,count_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkRDSuiw50gG",
        "outputId": "1c9185c5-641a-48e1-8a3f-7516eeacd0f6"
      },
      "source": [
        "import math\n",
        "for i, count in enumerate(countlist):\n",
        "    print(\"Top words in document {}\".format(i + 1))\n",
        "    scores = {word: tfidf(word, count, countlist) for word in count}\n",
        "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, score in sorted_words[:]:\n",
        "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top words in document 1\n",
            "\tWord: this, TF-IDF: 0.30099\n",
            "\tWord: time, TF-IDF: 0.30099\n",
            "\tWord: service, TF-IDF: 0.08917\n",
            "\tWord: terrible, TF-IDF: 0.02634\n",
            "Top words in document 2\n",
            "\tWord: service, TF-IDF: 0.11889\n",
            "\tWord: terrible, TF-IDF: 0.07024\n",
            "Top words in document 3\n",
            "\tWord: most, TF-IDF: 0.53648\n",
            "\tWord: service, TF-IDF: 0.11889\n",
            "\tWord: terrible, TF-IDF: 0.03512\n",
            "Top words in document 4\n",
            "\tWord: and, TF-IDF: 0.40236\n",
            "\tWord: experience, TF-IDF: 0.30099\n",
            "\tWord: service, TF-IDF: 0.08917\n",
            "\tWord: terrible, TF-IDF: 0.02634\n",
            "Top words in document 5\n",
            "\tWord: what, TF-IDF: 0.22907\n",
            "\tWord: a, TF-IDF: 0.22907\n",
            "\tWord: service, TF-IDF: 0.08917\n",
            "\tWord: terrible, TF-IDF: 0.02634\n",
            "Top words in document 6\n",
            "\tWord: so, TF-IDF: 0.30099\n",
            "\tWord: experience, TF-IDF: 0.30099\n",
            "\tWord: service, TF-IDF: 0.08917\n",
            "\tWord: terrible, TF-IDF: 0.02634\n",
            "Top words in document 7\n",
            "\tWord: disappointment, TF-IDF: 0.40236\n",
            "\tWord: what, TF-IDF: 0.22907\n",
            "\tWord: a, TF-IDF: 0.22907\n",
            "\tWord: terrible, TF-IDF: 0.02634\n",
            "Top words in document 8\n",
            "\tWord: place, TF-IDF: 0.40236\n",
            "\tWord: what, TF-IDF: 0.22907\n",
            "\tWord: a, TF-IDF: 0.22907\n",
            "\tWord: terrible, TF-IDF: 0.02634\n",
            "Top words in document 9\n",
            "\tWord: it, TF-IDF: 0.26824\n",
            "\tWord: this, TF-IDF: 0.20066\n",
            "\tWord: time, TF-IDF: 0.20066\n",
            "\tWord: was, TF-IDF: 0.20066\n",
            "\tWord: so, TF-IDF: 0.20066\n",
            "\tWord: horrible, TF-IDF: 0.20066\n",
            "Top words in document 10\n",
            "\tWord: the, TF-IDF: 0.40236\n",
            "\tWord: staff, TF-IDF: 0.40236\n",
            "\tWord: was, TF-IDF: 0.30099\n",
            "\tWord: horrible, TF-IDF: 0.30099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "Jw9ZdzrWaxEJ",
        "outputId": "3ff820ef-2b8f-4e5a-90c3-3c5909554bb2"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(countlist)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terrible</th>\n",
              "      <th>service</th>\n",
              "      <th>this</th>\n",
              "      <th>time</th>\n",
              "      <th>most</th>\n",
              "      <th>and</th>\n",
              "      <th>experience</th>\n",
              "      <th>what</th>\n",
              "      <th>a</th>\n",
              "      <th>so</th>\n",
              "      <th>disappointment</th>\n",
              "      <th>place</th>\n",
              "      <th>it</th>\n",
              "      <th>was</th>\n",
              "      <th>horrible</th>\n",
              "      <th>the</th>\n",
              "      <th>staff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   terrible  service  this  time  most  ...   it  was  horrible  the  staff\n",
              "0       1.0      1.0   1.0   1.0   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "1       2.0      1.0   NaN   NaN   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "2       1.0      1.0   NaN   NaN   1.0  ...  NaN  NaN       NaN  NaN    NaN\n",
              "3       1.0      1.0   NaN   NaN   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "4       1.0      1.0   NaN   NaN   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "5       1.0      1.0   NaN   NaN   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "6       1.0      NaN   NaN   NaN   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "7       1.0      NaN   NaN   NaN   NaN  ...  NaN  NaN       NaN  NaN    NaN\n",
              "8       NaN      NaN   1.0   1.0   NaN  ...  1.0  1.0       1.0  NaN    NaN\n",
              "9       NaN      NaN   NaN   NaN   NaN  ...  NaN  1.0       1.0  1.0    1.0\n",
              "\n",
              "[10 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGLQEuX8kU20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "99b1e164-abd3-45b5-f4e5-e1642a913332"
      },
      "source": [
        "import pandas as pd\n",
        "tf_idf_list = computTFIDF(countlist)\n",
        "df = pd.DataFrame(computTFIDF(countlist))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e2cc612a76eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_idf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'computTFIDF' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fislC_qDZsh"
      },
      "source": [
        "#以下是各種亂湊亂寫，但來不及寫出來QQ\n",
        "documentsa = documents[0]\n",
        "documentsb = documents[1]\n",
        "documentsc = documents[2]\n",
        "documentsd = documents[3]\n",
        "documentse = documents[4]\n",
        "documentsf = documents[5]\n",
        "documentsg = documents[6]\n",
        "documentsh = documents[7]\n",
        "documentsi = documents[8]\n",
        "documentsj = documents[9]\n",
        "#new_words = []\n",
        "#for item in documents:\n",
        "  #new_words.append(item)\n",
        "bowa = documentsa.split(' ')\n",
        "bowb = documentsb.split(' ')\n",
        "bowc = documentsc.split(' ')\n",
        "bowd = documentsd.split(' ')\n",
        "bowe = documentse.split(' ')\n",
        "bowf = documentsf.split(' ')\n",
        "bowg = documentsg.split(' ')\n",
        "bowh = documentsh.split(' ')\n",
        "bowi = documentsi.split(' ')\n",
        "bowj = documentsj.split(' ')\n",
        "uniquewords = set(bowa).union(set(bowb))\n",
        "uniquewords1 = set(bowb).union(set(bowc))\n",
        "uniquewords2 = set(bowc).union(set(bowd))\n",
        "uniquewords3 = set(bowd).union(set(bowe))\n",
        "numOfWordsA = dict.fromkeys(uniquewords,0)\n",
        "for word in bowa:\n",
        "    numOfWordsA[word] += 1\n",
        "numOfWordsB = dict.fromkeys(uniquewords, 0)\n",
        "for word in bowb:\n",
        "    numOfWordsB[word] += 1\n",
        "numOfWordsC = dict.fromkeys(uniquewords, 0)\n",
        "for word in bowc:\n",
        "    numOfWordsC[word] += 1\n",
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict\n",
        "tfA = computeTF(numOfWordsA, bowa)\n",
        "tfB = computeTF(numOfWordsB, bowb)\n",
        "tfC = computeTF(numOfWordsC, bowc)\n",
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict\n",
        "  idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "  def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf\n",
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ColNoXn68io_",
        "outputId": "e8e38a2c-c8c4-401d-acfb-e9ddba8a6192"
      },
      "source": [
        "str = \"\".join(documents)#list to str\n",
        "split = str.split()\n",
        "split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['terrible',\n",
              " 'service',\n",
              " 'this',\n",
              " 'timeterrible',\n",
              " 'terrible',\n",
              " 'servicemost',\n",
              " 'terrible',\n",
              " 'serviceterrible',\n",
              " 'service',\n",
              " 'and',\n",
              " 'experiencewhat',\n",
              " 'a',\n",
              " 'terrible',\n",
              " 'serviceso',\n",
              " 'terrible',\n",
              " 'service',\n",
              " 'experiencewhat',\n",
              " 'a',\n",
              " 'terrible',\n",
              " 'disappointmentwhat',\n",
              " 'a',\n",
              " 'terrible',\n",
              " 'placethis',\n",
              " 'time',\n",
              " 'it',\n",
              " 'was',\n",
              " 'so',\n",
              " 'horriblethe',\n",
              " 'staff',\n",
              " 'was',\n",
              " 'horrible']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZqe-Ujk-tmn",
        "outputId": "57d5d562-9da7-48be-aa3b-73718ff4cad7"
      },
      "source": [
        "# Find frequency of each word in a string in Python \n",
        "# using dictionary. \n",
        "   \n",
        "def count(elements): \n",
        "    # check if each word has '.' at its last. If so then ignore '.' \n",
        "    if elements[-1] == '.': \n",
        "        elements = elements[0:len(elements) - 1] \n",
        "   \n",
        "    # if there exists a key as \"elements\" then simply \n",
        "    # increase its value. \n",
        "    if elements in dictionary: \n",
        "        dictionary[elements] += 1\n",
        "   \n",
        "    # if the dictionary does not have the key as \"elements\"  \n",
        "    # then create a key \"elements\" and assign its value to 1. \n",
        "    else: \n",
        "        dictionary.update({elements: 1}) \n",
        "   \n",
        "   \n",
        "# driver input to check the program. \n",
        "   \n",
        "#Sentence = \"Apple Mango Orange Mango Guava Guava Mango\"\n",
        "   \n",
        "# Declare a dictionary \n",
        "dictionary = {} \n",
        "   \n",
        "# split all the word of the string. \n",
        "#lst = Sentence.split() \n",
        "   \n",
        "# take each word from lst and pass it to the method count. \n",
        "for elements in split: \n",
        "    count(elements) \n",
        "dictionary\n",
        "# print the keys and its corresponding values. \n",
        "#for allKeys in dictionary: \n",
        "    #print (\"Frequency of \", allKeys, end = \" \") \n",
        "    #print (\":\", end = \" \") \n",
        "    #print (dictionary[allKeys], end = \" \") \n",
        "    #print()  \n",
        "   \n",
        "# This code is contributed by Ronit Shrivastava\n",
        "#https://www.geeksforgeeks.org/find-frequency-of-each-word-in-a-string-in-python/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 3,\n",
              " 'and': 1,\n",
              " 'disappointment': 1,\n",
              " 'experience': 2,\n",
              " 'horrible': 2,\n",
              " 'it': 1,\n",
              " 'most': 1,\n",
              " 'place': 1,\n",
              " 'service': 6,\n",
              " 'so': 2,\n",
              " 'staff': 1,\n",
              " 'terrible': 9,\n",
              " 'the': 1,\n",
              " 'this': 2,\n",
              " 'time': 2,\n",
              " 'was': 2,\n",
              " 'what': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1J_bODkT0e"
      },
      "source": [
        "'''\n",
        "Answer here\n",
        "you can define other functions to support the defined function if you need.\n",
        "\n",
        "TF-IDF dataframe show as the following table.\n",
        "'''\n",
        "import numpy as np\n",
        "import math\n",
        "def computTFIDF(documents):\n",
        "\n",
        "  return \n",
        "#https://codertw.com/程式語言/363018/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgC7yFTBLjXv",
        "outputId": "eb3f1665-4103-48c5-cb5d-8ef4c23cbb00"
      },
      "source": [
        "#documents_array = np.array(documents)\n",
        "\n",
        "def nparray_tokenize(arr):\n",
        "  o = []\n",
        "  for x in arr:\n",
        "    o += x.split()\n",
        "    words = np.array(o) #type(documents) list to nparray\n",
        "    #unique_words = np.array(list(set(words.tolist())))\n",
        "  return o\n",
        "np_token = nparray_tokenize(documents)\n",
        "np_token\n",
        "#source:https://stackoverflow.com/questions/49874500/getting-words-out-of-a-numpy-array-of-sentence-strings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['terrible',\n",
              " 'service',\n",
              " 'this',\n",
              " 'time',\n",
              " 'terrible',\n",
              " 'terrible',\n",
              " 'service',\n",
              " 'most',\n",
              " 'terrible',\n",
              " 'service',\n",
              " 'terrible',\n",
              " 'service',\n",
              " 'and',\n",
              " 'experience',\n",
              " 'what',\n",
              " 'a',\n",
              " 'terrible',\n",
              " 'service',\n",
              " 'so',\n",
              " 'terrible',\n",
              " 'service',\n",
              " 'experience',\n",
              " 'what',\n",
              " 'a',\n",
              " 'terrible',\n",
              " 'disappointment',\n",
              " 'what',\n",
              " 'a',\n",
              " 'terrible',\n",
              " 'place',\n",
              " 'this',\n",
              " 'time',\n",
              " 'it',\n",
              " 'was',\n",
              " 'so',\n",
              " 'horrible',\n",
              " 'the',\n",
              " 'staff',\n",
              " 'was',\n",
              " 'horrible']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpmmKEdON7-w",
        "outputId": "6917f169-c1c5-47be-f645-c238fda21752"
      },
      "source": [
        "word, count = np.unique(np_token, return_counts=True)\n",
        "b = dict(zip(word, count))\n",
        "b\n",
        "#count27 = dict.items(b)\n",
        "#source:https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 3,\n",
              " 'and': 1,\n",
              " 'disappointment': 1,\n",
              " 'experience': 2,\n",
              " 'horrible': 2,\n",
              " 'it': 1,\n",
              " 'most': 1,\n",
              " 'place': 1,\n",
              " 'service': 6,\n",
              " 'so': 2,\n",
              " 'staff': 1,\n",
              " 'terrible': 9,\n",
              " 'the': 1,\n",
              " 'this': 2,\n",
              " 'time': 2,\n",
              " 'was': 2,\n",
              " 'what': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26rxvr4fqMRo"
      },
      "source": [
        "## 3-3. Define a scoring function to compute the cosine similarity between two input vector. (30%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIv3hr1MkVt0"
      },
      "source": [
        "'''\n",
        "Answer here\n",
        "return the cosine similarity between the given two vectors\n",
        "Apply the function which you designed to all sentences, and show your scoring results as the following table.\n",
        "'''\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from scipy import spatial\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def cosine_sim(vec_a, vec_b):\n",
        "#numpy version\n",
        "cos_sim = np.inner(dataSetI, dataSetII) / (norm(dataSetI) * norm(dataSetII))    #速度最快\n",
        "#cos_sim = dot(a, b)/(norm(a)*norm(b))       \n",
        "\n",
        "#scipy version\n",
        "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)    #速度一般般\n",
        "\n",
        "  return score\n",
        "寫不出來但有看了餘旋相似度的定理跟程式碼\n",
        "https://gist.github.com/DarKenW/16ad4174fd1bc3fb0111b091cee06f97\n",
        "https://www.cnblogs.com/dsgcBlogs/p/8619566.html\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2SxmzWqRF3"
      },
      "source": [
        "## 3-4. Show the cross comparation table for the given sentences. (10%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TYdcePcqcCB"
      },
      "source": [
        "cosine_sim()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}